{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sampling of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import argrelextrema, chirp, find_peaks, peak_widths\n",
    "from utils import smooth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "data_screw = pd.read_csv('../test_results/test_loaded.csv',\n",
    "                   delimiter=' ')\n",
    "\n",
    "data_screw_1 = pd.read_csv('../test_results/screw_test_1.csv',\n",
    "                   delimiter=' ')\n",
    "\n",
    "data_experiment = pd.read_csv('../test_results/test_joint.csv',\n",
    "                   delimiter=' ')\n",
    "\n",
    "data_dummy_normal = pd.read_csv('../test_results/dummy_normal.csv',\n",
    "                   delimiter=' ')\n",
    "\n",
    "data_dummy_anomaly = pd.read_csv('../test_results/dummy_anomaly.csv',\n",
    "                   delimiter=' ')\n",
    "\n",
    "data_normal = data_dummy_normal.iloc[:]\n",
    "data_anomaly = data_dummy_anomaly.iloc[:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "# NORMAL DATA\n",
    "# Find local maxima and minima\n",
    "indicator_sensor_normal = data_normal['target_qd_3'].values\n",
    "\n",
    "top_threshold_value = 0.5\n",
    "low_threshold_value = -0.5\n",
    "\n",
    "top_thresholded_normal = np.where(indicator_sensor_normal > top_threshold_value, indicator_sensor_normal, 0)\n",
    "low_thresholded_normal = np.where(indicator_sensor_normal < low_threshold_value, indicator_sensor_normal, 0)\n",
    "\n",
    "local_maxima_normal = argrelextrema(top_thresholded_normal, np.greater)[0]\n",
    "local_minima_normal = argrelextrema(low_thresholded_normal, np.less)[0]\n",
    "\n",
    "# Find corresponding samples in data\n",
    "df_maxima_normal = data_normal.iloc[local_maxima_normal]\n",
    "df_minima_normal = data_normal.iloc[local_minima_normal]\n",
    "\n",
    "print(len(df_maxima_normal.index))\n",
    "print(len(df_minima_normal.index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# ANOMALY DATA\n",
    "# Find local maxima and minima\n",
    "indicator_sensor_anomaly = data_anomaly['target_qd_3'].values\n",
    "\n",
    "top_threshold_value = 0.5\n",
    "low_threshold_value = -0.5\n",
    "\n",
    "top_thresholded_anomaly = np.where(indicator_sensor_anomaly > top_threshold_value, indicator_sensor_anomaly, 0)\n",
    "low_thresholded_anomaly = np.where(indicator_sensor_anomaly < low_threshold_value, indicator_sensor_anomaly, 0)\n",
    "\n",
    "local_maxima_anomaly = argrelextrema(top_thresholded_anomaly, np.greater)[0]\n",
    "local_minima_anomaly = argrelextrema(low_thresholded_anomaly, np.less)[0]\n",
    "\n",
    "# Find corresponding samples in data\n",
    "df_maxima_anomaly = data_anomaly.iloc[local_maxima_anomaly]\n",
    "df_minima_anomaly = data_anomaly.iloc[local_minima_anomaly]\n",
    "\n",
    "print(len(df_maxima_anomaly.index))\n",
    "print(len(df_minima_anomaly.index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "# # TODO: Daniella will do second derivatives here in order to find the whole samples.\n",
    "# joint_accelerations = data['target_qdd_3'].values\n",
    "# smoothed_indicator_sensor = smooth(indicator_sensor, 30)\n",
    "# #joint_accelerations = np.gradient(indicator_sensor)\n",
    "#\n",
    "# min_val = joint_accelerations.min() * 5\n",
    "# max_val = joint_accelerations.max() * 5\n",
    "# #joint_accelerations = np.interp(joint_accelerations, (joint_accelerations.min(), joint_accelerations.max()), (min_val, max_val))\n",
    "#\n",
    "# t_indicator_sensor = np.vstack((data['timestamp'], indicator_sensor))\n",
    "# t_joint_accelerations = np.vstack((data['timestamp'], joint_accelerations))\n",
    "# idx = np.argwhere(np.diff(np.sign(indicator_sensor - joint_accelerations))).flatten()\n",
    "#\n",
    "#\n",
    "# %matplotlib qt\n",
    "# plt.plot(indicator_sensor,color='red')\n",
    "# plt.plot(joint_accelerations,'blue')\n",
    "# #plt.scatter(local_maxima, joint_accelerations[local_maxima])\n",
    "# #plt.scatter(local_minima, joint_accelerations[local_minima])\n",
    "# plt.scatter(idx, indicator_sensor[idx])\n",
    "# plt.plot()\n",
    "#\n",
    "# plt.legend({'target_qdd_3', 'target_qd_3'})\n",
    "# plt.show()\n",
    "#\n",
    "# print(joint_accelerations[local_maxima])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "# Create time-series samples of normal data\n",
    "samples_list_normal = []\n",
    "i = 0\n",
    "for index, row in df_maxima_normal.iterrows():\n",
    "    sample = data_normal.iloc[df_minima_normal.iloc[i].name:index]\n",
    "    samples_list_normal.append(sample)\n",
    "    i += 1\n",
    "single_sample = samples_list_normal[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "# Create time-series samples of anomaly data\n",
    "samples_list_anomaly = []\n",
    "i = 0\n",
    "for index, row in df_maxima_anomaly.iterrows():\n",
    "    sample = data_anomaly.iloc[df_minima_anomaly.iloc[i].name:index]\n",
    "    samples_list_anomaly.append(sample)\n",
    "    i += 1\n",
    "single_sample = samples_list_anomaly[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Measure lengths of the normal samples\n",
    "samples_length_normal = []\n",
    "for sample in samples_list_normal:\n",
    "    samples_length_normal.append(len(sample.index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Measure lengths of the anomaly samples\n",
    "samples_length_anomaly = []\n",
    "for sample in samples_list_anomaly:\n",
    "    samples_length_anomaly.append(len(sample.index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Delete the incorrect samples\n",
    "samples_list_normal = [x for x in samples_list_normal if len(x.index) <= 281]\n",
    "samples_length_normal = [x for x in samples_length_normal if x <= 281]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Delete the incorrect samples\n",
    "samples_list_anomaly = [x for x in samples_list_anomaly if len(x.index) <= 87]\n",
    "samples_length_anomaly = [x for x in samples_length_anomaly if x <= 87]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finishing analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set plotting data\n",
    "data = data_anomaly\n",
    "df_maxima = df_maxima_anomaly\n",
    "df_minima = df_minima_anomaly\n",
    "samples_list = samples_list_anomaly\n",
    "samples_length = samples_length_anomaly\n",
    "\n",
    "# data = data_normal\n",
    "# df_maxima = df_maxima_normal\n",
    "# df_minima = df_minima_normal\n",
    "# samples_list = samples_list_normal\n",
    "# samples_length = samples_length_normal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get positions of the longest and shortest samples\n",
    "longest_sample_pos = samples_length.index(max(samples_length))\n",
    "shortest_sample_pos = samples_length.index(min(samples_length))\n",
    "\n",
    "samples_length = np.unique(samples_length)\n",
    "\n",
    "print('Dataset contains {n_samples} samples'.format(\n",
    "    n_samples=len(samples_list)))\n",
    "\n",
    "print('Samples have lengths of {samples_length}'.format(\n",
    "    samples_length=samples_length\n",
    "))\n",
    "\n",
    "longest_sample = samples_list[longest_sample_pos]\n",
    "shortest_sample = samples_list[shortest_sample_pos]\n",
    "\n",
    "length_comparison = [shortest_sample, longest_sample]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the selected data and shade all samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# #plt.plot(data['timestamp'], data['target_q_0'],\n",
    "# #         label='Line 0')\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_1'],\n",
    "# #        label='Line 1')\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_2'],\n",
    "# #        label='Line 2')\n",
    "#\n",
    "# # Plot the values for joint responsible for movement in axis Z\n",
    "# plt.plot(data['timestamp'], data['target_q_3'],\n",
    "#         label='Line 3')\n",
    "#\n",
    "# plt.plot(data['timestamp'], data['target_qd_3'],\n",
    "#         label='Line 4')\n",
    "#\n",
    "# # Plot the found local maxima\n",
    "# for index, row in df_maxima.iterrows():\n",
    "#     plt.axvline(x=row['timestamp'],\n",
    "#                 color='green')\n",
    "#\n",
    "# # Plot the found local minima\n",
    "# for index, row in df_minima.iterrows():\n",
    "#     plt.axvline(x=row['timestamp'],\n",
    "#                 color='red')\n",
    "#\n",
    "# # Plot the thresholds\n",
    "# plt.axhline(top_threshold_value,\n",
    "#            color='green',\n",
    "#             linestyle='--')\n",
    "#\n",
    "# plt.axhline(low_threshold_value,\n",
    "#            color='red',\n",
    "#            linestyle='--')\n",
    "#\n",
    "# # # Shade the samples\n",
    "# # for sample in samples_list:\n",
    "# #     plt.axvspan(sample.head(1)['timestamp'].values[0],\n",
    "# #                 sample.tail(1)['timestamp'].values[0],\n",
    "# #                 facecolor='b', alpha=0.1)\n",
    "#\n",
    "# # plt.tight_layout()\n",
    "# # plt.autoscale(enable=True, axis='x')\n",
    "# # plt.autoscale(enable=True, axis='y')\n",
    "# # plt.legend()\n",
    "# # plt.show()\n",
    "#\n",
    "# # # Find the peaks\n",
    "# # peaks, _ = find_peaks(indicator_sensor)\n",
    "# #\n",
    "# # results_half = peak_widths(indicator_sensor, peaks, rel_height=0.5)\n",
    "# # results_full = peak_widths(indicator_sensor, peaks, rel_height=0.7)\n",
    "# #\n",
    "# # plt.plot(indicator_sensor)\n",
    "# #\n",
    "# # plt.plot(peaks, indicator_sensor[peaks], \"x\")\n",
    "# # plt.plot(data['target_q_3'])\n",
    "# #\n",
    "# # plt.hlines(*results_half[1:], color=\"C2\")\n",
    "# #\n",
    "# # plt.hlines(*results_full[1:], color=\"C3\")\n",
    "#\n",
    "# # Shade the samples\n",
    "# for sample in samples_list:\n",
    "#     plt.axvspan(sample.head(1)['timestamp'].values[0],\n",
    "#                 sample.tail(1)['timestamp'].values[0],\n",
    "#                 facecolor='b', alpha=0.1)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.autoscale(enable=True, axis='x')\n",
    "# plt.autoscale(enable=True, axis='y')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot comparison between the longest and shortest samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_0'],\n",
    "# #         label='Line 0')\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_1'],\n",
    "# #        label='Line 1')\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_2'],\n",
    "# #        label='Line 2')\n",
    "#\n",
    "# # Plot the values for joint responsible for movement in axis Z\n",
    "# plt.plot(data['timestamp'], data['target_q_3'],\n",
    "#         label='Line 3')\n",
    "#\n",
    "# plt.plot(data['timestamp'], data['target_qd_3'],\n",
    "#         label='Line 4')\n",
    "#\n",
    "# # Plot the thresholds\n",
    "# plt.axhline(top_threshold_value,\n",
    "#            color='green',\n",
    "#             linestyle='--')\n",
    "#\n",
    "# plt.axhline(low_threshold_value,\n",
    "#            color='red',\n",
    "#            linestyle='--')\n",
    "#\n",
    "# # Shade the longest and shortest samples\n",
    "# for sample in length_comparison:\n",
    "#     plt.axvspan(sample.head(1)['timestamp'].values[0],\n",
    "#                 sample.tail(1)['timestamp'].values[0],\n",
    "#                 facecolor='b', alpha=0.1)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.autoscale(enable=True, axis='x')\n",
    "# plt.autoscale(enable=True, axis='y')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mix the dataset and save the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Delete timestamp column and pad samples to be the same shape\n",
    "# TODO: Very inefficient\n",
    "for id, sample in enumerate(samples_list_normal):\n",
    "    sample = sample.drop(['timestamp'], axis=1)\n",
    "    while len(sample.index) < max(samples_length_normal):\n",
    "        sample = sample.append(pd.Series(0, index=sample.columns), ignore_index=True)\n",
    "    samples_list_normal[id] = sample\n",
    "    samples_list_normal[id]['label'] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Delete timestamp column and pad samples to be the same shape\n",
    "# TODO: Very inefficient\n",
    "for id, sample in enumerate(samples_list_anomaly):\n",
    "    sample = sample.drop(['timestamp'], axis=1)\n",
    "    while len(sample.index) < max(samples_length_normal):\n",
    "        sample = sample.append(pd.Series(0, index=sample.columns), ignore_index=True)\n",
    "    samples_list_anomaly[id] = sample\n",
    "    samples_list_anomaly[id]['label'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of samples\n",
    "training_dictionary = {}\n",
    "for id, sample in enumerate(samples_list_normal):\n",
    "    training_dictionary.update({id: sample})\n",
    "\n",
    "# Concatenate samples\n",
    "training_dataset = pd.concat(training_dictionary)\n",
    "training_dataset.index.names = ['sample', 'event']\n",
    "\n",
    "# Save samples\n",
    "training_dataset.to_pickle('../datasets/dummy/training.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of samples\n",
    "test_dictionary = training_dictionary\n",
    "for id, sample in enumerate(samples_list_anomaly):\n",
    "    dict_id = id + len(samples_list_normal) + 1\n",
    "    test_dictionary.update({dict_id: sample})\n",
    "\n",
    "# Concatenate samples\n",
    "test_dataset = pd.concat(test_dictionary)\n",
    "test_dataset.index.names = ['sample', 'event']\n",
    "\n",
    "# Save samples\n",
    "test_dataset.to_pickle('../datasets/dummy/test.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the training dataset and make numpy array out of it\n",
    "loaded_dataset = pd.read_pickle('../datasets/dummy/training.pkl')\n",
    "\n",
    "# Extract the labels and create a samples vector out of it\n",
    "labels = loaded_dataset.iloc[:, loaded_dataset.columns.get_level_values(0) == 'label']\n",
    "labels = labels.droplevel('event')\n",
    "labels = labels[~labels.index.duplicated(keep='first')]\n",
    "labels_np = np.squeeze(labels.values)\n",
    "\n",
    "# Drop the labels from data\n",
    "loaded_dataset = loaded_dataset.drop('label', axis=1)\n",
    "\n",
    "dim_0 = len(loaded_dataset.index.get_level_values(0).unique())\n",
    "dim_1 = int(len(loaded_dataset.index.get_level_values(1)) / dim_0)\n",
    "dim_2 = loaded_dataset.shape[1]\n",
    "\n",
    "loaded_dataset_np = loaded_dataset.values.reshape((dim_0, dim_1, dim_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "# Load the training dataset and make numpy array out of it\n",
    "loaded_dataset = pd.read_pickle('../datasets/dummy/training.pkl')\n",
    "\n",
    "# Extract the labels and create a samples vector out of it\n",
    "labels = loaded_dataset.iloc[:, loaded_dataset.columns.get_level_values(0) == 'label']\n",
    "labels = labels.droplevel('event')\n",
    "labels = labels[~labels.index.duplicated(keep='first')]\n",
    "labels_np = np.squeeze(labels.values)\n",
    "\n",
    "# Drop the labels from data\n",
    "loaded_dataset = loaded_dataset.drop('label', axis=1)\n",
    "\n",
    "dim_0 = len(loaded_dataset.index.get_level_values(0).unique())\n",
    "dim_1 = int(len(loaded_dataset.index.get_level_values(1)) / dim_0)\n",
    "dim_2 = loaded_dataset.shape[1]\n",
    "\n",
    "loaded_dataset_np = loaded_dataset.values.reshape((dim_0, dim_1, dim_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "# Measure lengths of the anomaly samples\n",
    "samples_length_anomaly = []\n",
    "for sample in samples_list_anomaly:\n",
    "    samples_length_anomaly.append(len(sample.index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "# Delete the incorrect samples\n",
    "samples_list_normal = [x for x in samples_list_normal if len(x.index) <= 281]\n",
    "samples_length_normal = [x for x in samples_length_normal if x <= 281]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "# Delete the incorrect samples\n",
    "samples_list_anomaly = [x for x in samples_list_anomaly if len(x.index) <= 87]\n",
    "samples_length_anomaly = [x for x in samples_length_anomaly if x <= 87]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finishing analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "# Set plotting data\n",
    "data = data_anomaly\n",
    "df_maxima = df_maxima_anomaly\n",
    "df_minima = df_minima_anomaly\n",
    "samples_list = samples_list_anomaly\n",
    "samples_length = samples_length_anomaly\n",
    "\n",
    "# data = data_normal\n",
    "# df_maxima = df_maxima_normal\n",
    "# df_minima = df_minima_normal\n",
    "# samples_list = samples_list_normal\n",
    "# samples_length = samples_length_normal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 76 samples\n",
      "Samples have lengths of [79 80 85 86 87]\n"
     ]
    }
   ],
   "source": [
    "# Get positions of the longest and shortest samples\n",
    "longest_sample_pos = samples_length.index(max(samples_length))\n",
    "shortest_sample_pos = samples_length.index(min(samples_length))\n",
    "\n",
    "samples_length = np.unique(samples_length)\n",
    "\n",
    "print('Dataset contains {n_samples} samples'.format(\n",
    "    n_samples=len(samples_list)))\n",
    "\n",
    "print('Samples have lengths of {samples_length}'.format(\n",
    "    samples_length=samples_length\n",
    "))\n",
    "\n",
    "longest_sample = samples_list[longest_sample_pos]\n",
    "shortest_sample = samples_list[shortest_sample_pos]\n",
    "\n",
    "length_comparison = [shortest_sample, longest_sample]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the selected data and shade all samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# #plt.plot(data['timestamp'], data['target_q_0'],\n",
    "# #         label='Line 0')\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_1'],\n",
    "# #        label='Line 1')\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_2'],\n",
    "# #        label='Line 2')\n",
    "#\n",
    "# # Plot the values for joint responsible for movement in axis Z\n",
    "# plt.plot(data['timestamp'], data['target_q_3'],\n",
    "#         label='Line 3')\n",
    "#\n",
    "# plt.plot(data['timestamp'], data['target_qd_3'],\n",
    "#         label='Line 4')\n",
    "#\n",
    "# # Plot the found local maxima\n",
    "# for index, row in df_maxima.iterrows():\n",
    "#     plt.axvline(x=row['timestamp'],\n",
    "#                 color='green')\n",
    "#\n",
    "# # Plot the found local minima\n",
    "# for index, row in df_minima.iterrows():\n",
    "#     plt.axvline(x=row['timestamp'],\n",
    "#                 color='red')\n",
    "#\n",
    "# # Plot the thresholds\n",
    "# plt.axhline(top_threshold_value,\n",
    "#            color='green',\n",
    "#             linestyle='--')\n",
    "#\n",
    "# plt.axhline(low_threshold_value,\n",
    "#            color='red',\n",
    "#            linestyle='--')\n",
    "#\n",
    "# # # Shade the samples\n",
    "# # for sample in samples_list:\n",
    "# #     plt.axvspan(sample.head(1)['timestamp'].values[0],\n",
    "# #                 sample.tail(1)['timestamp'].values[0],\n",
    "# #                 facecolor='b', alpha=0.1)\n",
    "#\n",
    "# # plt.tight_layout()\n",
    "# # plt.autoscale(enable=True, axis='x')\n",
    "# # plt.autoscale(enable=True, axis='y')\n",
    "# # plt.legend()\n",
    "# # plt.show()\n",
    "#\n",
    "# # # Find the peaks\n",
    "# # peaks, _ = find_peaks(indicator_sensor)\n",
    "# #\n",
    "# # results_half = peak_widths(indicator_sensor, peaks, rel_height=0.5)\n",
    "# # results_full = peak_widths(indicator_sensor, peaks, rel_height=0.7)\n",
    "# #\n",
    "# # plt.plot(indicator_sensor)\n",
    "# #\n",
    "# # plt.plot(peaks, indicator_sensor[peaks], \"x\")\n",
    "# # plt.plot(data['target_q_3'])\n",
    "# #\n",
    "# # plt.hlines(*results_half[1:], color=\"C2\")\n",
    "# #\n",
    "# # plt.hlines(*results_full[1:], color=\"C3\")\n",
    "#\n",
    "# # Shade the samples\n",
    "# for sample in samples_list:\n",
    "#     plt.axvspan(sample.head(1)['timestamp'].values[0],\n",
    "#                 sample.tail(1)['timestamp'].values[0],\n",
    "#                 facecolor='b', alpha=0.1)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.autoscale(enable=True, axis='x')\n",
    "# plt.autoscale(enable=True, axis='y')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot comparison between the longest and shortest samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_0'],\n",
    "# #         label='Line 0')\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_1'],\n",
    "# #        label='Line 1')\n",
    "#\n",
    "# #plt.plot(data['timestamp'], data['target_q_2'],\n",
    "# #        label='Line 2')\n",
    "#\n",
    "# # Plot the values for joint responsible for movement in axis Z\n",
    "# plt.plot(data['timestamp'], data['target_q_3'],\n",
    "#         label='Line 3')\n",
    "#\n",
    "# plt.plot(data['timestamp'], data['target_qd_3'],\n",
    "#         label='Line 4')\n",
    "#\n",
    "# # Plot the thresholds\n",
    "# plt.axhline(top_threshold_value,\n",
    "#            color='green',\n",
    "#             linestyle='--')\n",
    "#\n",
    "# plt.axhline(low_threshold_value,\n",
    "#            color='red',\n",
    "#            linestyle='--')\n",
    "#\n",
    "# # Shade the longest and shortest samples\n",
    "# for sample in length_comparison:\n",
    "#     plt.axvspan(sample.head(1)['timestamp'].values[0],\n",
    "#                 sample.tail(1)['timestamp'].values[0],\n",
    "#                 facecolor='b', alpha=0.1)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.autoscale(enable=True, axis='x')\n",
    "# plt.autoscale(enable=True, axis='y')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mix the dataset and save the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "# Delete timestamp column and pad samples to be the same shape\n",
    "# TODO: Very inefficient\n",
    "for id, sample in enumerate(samples_list_normal):\n",
    "    sample = sample.drop(['timestamp'], axis=1)\n",
    "    while len(sample.index) < max(samples_length_normal):\n",
    "        sample = sample.append(pd.Series(0, index=sample.columns), ignore_index=True)\n",
    "    samples_list_normal[id] = sample\n",
    "    samples_list_normal[id]['label'] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [],
   "source": [
    "# Delete timestamp column and pad samples to be the same shape\n",
    "# TODO: Very inefficient\n",
    "for id, sample in enumerate(samples_list_anomaly):\n",
    "    sample = sample.drop(['timestamp'], axis=1)\n",
    "    while len(sample.index) < max(samples_length_normal):\n",
    "        sample = sample.append(pd.Series(0, index=sample.columns), ignore_index=True)\n",
    "    samples_list_anomaly[id] = sample\n",
    "    samples_list_anomaly[id]['label'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [],
   "source": [
    "# Create dictionary of samples\n",
    "training_dictionary = {}\n",
    "for id, sample in enumerate(samples_list_normal):\n",
    "    training_dictionary.update({id: sample})\n",
    "\n",
    "# Concatenate samples\n",
    "training_dataset = pd.concat(training_dictionary)\n",
    "training_dataset.index.names = ['sample', 'event']\n",
    "\n",
    "# Save samples\n",
    "training_dataset.to_pickle('../datasets/dummy/training.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [],
   "source": [
    "# Create dictionary of samples\n",
    "test_dictionary = training_dictionary\n",
    "for id, sample in enumerate(samples_list_anomaly):\n",
    "    dict_id = id + len(samples_list_normal) + 1\n",
    "    test_dictionary.update({dict_id: sample})\n",
    "\n",
    "# Concatenate samples\n",
    "test_dataset = pd.concat(test_dictionary)\n",
    "test_dataset.index.names = ['sample', 'event']\n",
    "\n",
    "# Save samples\n",
    "test_dataset.to_pickle('../datasets/dummy/test.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "# Load the training dataset and make numpy array out of it\n",
    "loaded_dataset = pd.read_pickle('../datasets/dummy/training.pkl')\n",
    "\n",
    "# Extract the labels and create a samples vector out of it\n",
    "labels = loaded_dataset.iloc[:, loaded_dataset.columns.get_level_values(0) == 'label']\n",
    "labels = labels.droplevel('event')\n",
    "labels = labels[~labels.index.duplicated(keep='first')]\n",
    "labels_np = np.squeeze(labels.values)\n",
    "\n",
    "# Drop the labels from data\n",
    "loaded_dataset = loaded_dataset.drop('label', axis=1)\n",
    "\n",
    "dim_0 = len(loaded_dataset.index.get_level_values(0).unique())\n",
    "dim_1 = int(len(loaded_dataset.index.get_level_values(1)) / dim_0)\n",
    "dim_2 = loaded_dataset.shape[1]\n",
    "\n",
    "loaded_dataset_np = loaded_dataset.values.reshape((dim_0, dim_1, dim_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "anomaly_detection",
   "language": "python",
   "display_name": "anomaly_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}